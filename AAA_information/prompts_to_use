
# ENVIROMENT:
- Always use the venv310 activated 

# CREDENTIALS
To actualize the credentials look at : 
- /Users/arriazui/Desktop/GreenComputing/flexecutor-main/aws_credentials.md 

# PODMAN
Docker is not allowed, use Podman instead of docker, to build the runtime use always the lithops process, with something like this :
export DOCKER_ENGINE=podman && lithops runtime build -b aws_lambda -f examples/titanic/Dockerfile.arm64 titanic_aws_lambda_arm_greencomp

# LITHOPS_CONFIG_FILE : 
export LITHOPS_CONFIG_FILE=/Users/arriazui/Desktop/GreenComputing/flexecutor-main/config_aws.yaml

The last version of lithops, actualized and always use and install should be : 
/Users/arriazui/Desktop/GreenComputing/lithops_fork 

# IMPORTANT: 
use this to create / manage runtimes : 
/Users/arriazui/Desktop/GreenComputing/flexecutor-main/AAA_information/S_Docker_build.md

# With that in mind now INSTRUCTIONS: 
could you creaate a new runtime in base what i mention called titanic_aws_lambda_arm_greencomp_v1_512mb_v2 
where the runtime in aws is of 512Mb  for this file /Users/arriazui/Desktop/GreenComputing/flexecutor-main/examples/titanic/main_batch_execution.py





# problems identified : 
üö® ROOT CAUSE ANALYSIS - S3 Cleanup Failure
üîç Files Currently in S3 Bucket:
accuracies/: 9 files (21 bytes each)
forests/: 86+ files (6MB to 34MB each) - These are HUGE!
lithops.jobs/: Multiple job files (46MB each) + status files
models/: 300+ files (1.5MB each)
predictions/: 77+ files (500KB each)
training-data-transform/: 4 files (18MB each)
vectors-pca/: 4 files (255KB each)
üí∞ Storage Impact:
Total estimated size: Over 2GB of accumulated temporary files!
Cost impact: Significant storage costs accumulating
