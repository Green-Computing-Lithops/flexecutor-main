{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedcb79d",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning grid search example\n",
    "\n",
    "In this notebook, hyperparameter tuning using grid search algorithm is demonstrated.We have a dataset consisting\n",
    "of amazon product reviews and a sklearn classifier to classiy these reviews. We take advantage of cloud functions\n",
    "to tune this classifier's hyperparameters and show how Lithops can be used for this kind of computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0fd11",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f22288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8003720",
   "metadata": {},
   "source": [
    "## Downloading the Dataset\n",
    "\n",
    "The dataset should be downloaded and extracted from zip file before this step. \n",
    "`load_data` function seperates the data as X and Y arrays to prepare them for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101ae743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"bittlingmayer/amazonreviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb189d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/german/.cache/kagglehub/datasets/bittlingmayer/amazonreviews/versions/7\"\n",
    "def load_data(mib):\n",
    "    # Download the dataset at\n",
    "    # https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "\n",
    "    print(\"Loading Amazon reviews dataset:\")\n",
    "    compressed = bz2.BZ2File(path + \"/\" + \"train.ft.txt.bz2\")\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    total_size = 0\n",
    "    for _ in range(3_600_000):\n",
    "        line = compressed.readline().decode('utf-8')\n",
    "        X.append(line[11:])\n",
    "        y.append(int(line[9]) - 1)  # __label__1, __label__2\n",
    "\n",
    "        total_size += len(line[11:])\n",
    "        if (total_size / 2 ** 20) > mib:\n",
    "            break\n",
    "\n",
    "    print(\"\\t%d reviews\" % len(X))\n",
    "    print(\"\\t%0.2f MiB of data\" % (total_size / 2 ** 20))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169baf5",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "In the main function, grid search is performed using GridSearchCV from sklearn library with different parameters depending on the backend chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159dd39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(backend, address, mib, refit, jobs):\n",
    "\n",
    "    X, y = load_data(mib)\n",
    "\n",
    "    n_features = 2 ** 18\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', HashingVectorizer(n_features=n_features, alternate_sign=False)),\n",
    "        ('clf', SGDClassifier()),\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        'vect__norm': ('l1', 'l2'),\n",
    "        'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'clf__alpha': (1e-2, 1e-3, 1e-4, 1e-5),\n",
    "        'clf__max_iter': (20, 60, 100, 160),\n",
    "        'clf__penalty': ('l2', 'l1', 'elasticnet')\n",
    "    }\n",
    "\n",
    "    if backend == 'lithops':\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from lithops.util.joblib import register_lithops\n",
    "        register_lithops()\n",
    "        grid_search = GridSearchCV(pipeline, parameters,\n",
    "                                   error_score='raise',\n",
    "                                   refit=refit, cv=5, n_jobs=jobs)\n",
    "\n",
    "    elif backend == 'ray':\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        import ray\n",
    "        from ray.util.joblib import register_ray\n",
    "        address = 'auto' if address is None else address\n",
    "        ray.init(address, redis_password='5241590000000000')\n",
    "        register_ray()\n",
    "        grid_search = GridSearchCV(pipeline, parameters,\n",
    "                                   error_score='raise',\n",
    "                                   refit=refit, cv=5, n_jobs=jobs)\n",
    "\n",
    "    elif backend == 'tune':\n",
    "        from tune_sklearn import TuneGridSearchCV\n",
    "        import ray\n",
    "        address = 'auto' if address is None else address\n",
    "        ray.init(address, log_to_driver=False, redis_password='5241590000000000')\n",
    "        grid_search = TuneGridSearchCV(pipeline, parameters,\n",
    "            error_score='raise', refit=refit, cv=5, n_jobs=jobs)\n",
    "        backend = 'loky' # not used\n",
    "\n",
    "    elif backend == 'dask':\n",
    "        from dask_ml.model_selection import GridSearchCV\n",
    "        from dask_ml.feature_extraction.text import HashingVectorizer as DaskHashingVectorizer\n",
    "        from distributed import Client\n",
    "        if address is None:\n",
    "            print('Error: must specify a scheduler address for dask distributed')\n",
    "            exit(1)\n",
    "        Client(address=address)\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', DaskHashingVectorizer(n_features=n_features, alternate_sign=False)),\n",
    "            ('clf', SGDClassifier()),\n",
    "        ])\n",
    "        grid_search = GridSearchCV(pipeline, parameters,\n",
    "            error_score='raise', refit=refit, cv=5, n_jobs=jobs)\n",
    "\n",
    "    else:   # loky\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        grid_search = GridSearchCV(pipeline, parameters,\n",
    "            error_score='raise', refit=refit, cv=5, n_jobs=jobs)\n",
    "\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters: \", end='')\n",
    "    pprint(parameters)\n",
    "\n",
    "    with joblib.parallel_backend(backend):\n",
    "        print(\"Performing grid search...\")\n",
    "        t0 = time()\n",
    "        grid_search.fit(X, y)\n",
    "        total_time = time() - t0\n",
    "        print(\"Done in %0.3fs\\n\" % total_time) \n",
    "\n",
    "    if refit:\n",
    "        print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "        print(\"Best parameters set:\")\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main(\"lithops\", None, 10, False, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
